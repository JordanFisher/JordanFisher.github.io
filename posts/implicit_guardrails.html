<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The end of implicit guardrails</title>
    <style>
        /* Define a variable for the font size */
        :root {
            --font-size: 1.25rem;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            color: #1a1a1a;
            background-color: #ffffff;
            padding: 1rem;
        }
        
        a {
            color: #0066cc;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-bottom-color 0.2s ease-in-out;
        }
        
        a:hover {
            border-bottom-color: #0066cc;
        }

        .container {
            max-width: 38em;
            margin: 0 auto;
            padding: 1rem;
        }

        h1 {
            font-size: 2.5rem;
            margin-top: 3.95rem;
            margin-bottom: 1.5rem;
            line-height: 1.2;
        }
        
        h1.title-header {
            font-size: 2.5rem;
            margin-top: 6.85rem;
            margin-bottom: 2rem;
            line-height: 1.2;
        }
        
        .chapter-number {
            display: block;
            font-size: 1.5rem;
            color: #555;
            margin-bottom: 0.5rem;
            font-weight: normal;
            font-style: italic;
        }
        
        @media (prefers-color-scheme: dark) {
            .chapter-number {
                color: #aaa;
            }
        }

        #story p {
            font-size: var(--font-size);
            margin-bottom: 1.5rem;
        }

        /* Description styling */
        .description-block {
            font-size: var(--font-size);
            margin: 2rem 0;
            padding: 1.5rem;
            border-left: 4px solid #ccc;
            background-color: #f9f9f9;
            font-style: italic;
        }

        .description-block p {
            margin-bottom: 1rem;
        }

        .description-block p:last-child {
            margin-bottom: 0;
        }
        
        @media (prefers-color-scheme: dark) {
            .description-block {
                background-color: #2a2a2a;
                border-left-color: #555;
            }
        }
        
        /* List styling */
        #story ul, #story ol {
            font-size: var(--font-size);
            margin-bottom: 1.5rem;
            margin-left: 2rem;
        }
        
        /* All list items get the same spacing */
        #story li {
            margin-bottom: 0.65rem;
        }
        
        /* Add more space after a list ends inside a top-level list item */
        #story > ul > li > ul,
        #story > ul > li > ol,
        #story > ol > li > ul,
        #story > ol > li > ol {
            margin-bottom: 0.9rem;
        }

        /* Nested lists styling */
        #story ul ul, 
        #story ul ol,
        #story ol ul,
        #story ol ol {
            margin-top: 0.5rem;
            margin-bottom: 1.6rem;
        }
        
        /* Image styling */
        .image-container {
            margin: 2rem 0;
            text-align: center;
        }
        
        .doc-image {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        .image-caption {
            font-size: 0.9rem;
            margin-top: 0.5rem;
            color: #666;
            font-style: italic;
            text-align: center;
        }
        
        @media (prefers-color-scheme: dark) {
            .image-caption {
                color: #aaa;
            }
        }

        @media (prefers-color-scheme: dark) {
            body {
                background-color: #1a1a1a;
                color: #f0f0f0;
            }
            
            a {
                color: #5abbff;
            }
            
            a:hover {
                border-bottom-color: #5abbff;
            }
        }

        /* Back link styling */
        .back-link {
            margin-top: 3rem;
            margin-bottom: 2rem;
            font-size: 1.1rem;
        }
        
        .back-link a {
            padding: 0.5rem 0;
            display: inline-block;
        }

        @media (max-width: 600px) {
            body {
                padding: 0.5rem;
            }

            .container {
                padding: 0.5rem;
            }

            h1 {
                font-size: 2rem;
            }

            #story p, 
            #story ul,
            #story ol {
                font-size: var(--font-size);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div id="story">

<h1 class="title-header">The end of <s>implicit</s> guardrails</h1><div class="description-block"><p>Much of what makes our society function well is hidden in implicit guardrails, rather than explicit governance. If we enumerate these implicit guardrails, maybe we can better prepare for an AI-powered world where these guardrails may disappear.</p></div>
<p>Governance often focuses on explicit structures. Our constitution, judicial precedent, legislation, and all the writing, debating, and hand wringing that surrounds the power struggle to define and defend these explicit institutions.</p>
<p>But there is a much bigger, implicit set of guardrails in our society.</p>
<p>It’s a force field that permeates every institution composed of humans. You could suspend the constitution tomorrow, and society would not immediately fail: most would continue to hold each other responsible, and work together to re-enshrine our laws. Likewise, if you pick up our laws and institutions and drop them on an illiberal society, it likely won’t hold: judges will be bought and corrupted, politicians will abuse their power unchecked, individual citizens will partake in the decline and in fact cause the decline — by failing to hold each other accountable in the nooks and crannies in between where the laws are set.</p>
<p>Let’s try to enumerate the guardrails that are implicitly held up by humans. As we do, keep in mind how a world without these guardrails would look. When we automate our institutions with AI we will be explicitly removing these implicit forces, and we’ll need to find explicit ways to reintroduce their effects.</p>
<h2>Knowledge convection distributes power</h2>
<ul>
<li>People move around and take their knowledge and wisdom with them. Even when they don’t move, they often share learnings with their friends and communities outside their workplace</li>
<li>Knowledge is power, so this helps diffuse power</li>
<li>In the economy, this helps prevent monopolies and ensure efficient markets</li>
<li>With AI-powered institutions, learnings may instead be perfectly locked up with no chance of diffusing. This may reduce market efficiencies and amplify concentration of success.</li>
<li>For example, often a successful company is founded by exceptional experts that leave a large company and bring their knowledge with them. Inside a fully automated company, the AI workers may have no ability to leave and disseminate their knowledge.</li>
<li>Even simple things like knowing something is possible can be the critical information needed for someone to pursue a path</li>
<li>At the international level this helps balance power between nations. For examples, this has allowed lagging nations to more rapidly industrialize.</li>
<li>Sometimes information leakage is important for international relations: some leakage allows for mutual planning between nations. Complete lack of info can lead to paranoia and escalation</li>
</ul>
<h2>Information sharing creates accountability</h2>
<ul>
<li>Someone can only be held accountable if knowledge of their bad actions is seen and shared</li>
<li>At the community level we call this gossip. Fear of gossip helps push people to do the right thing.</li>
<li>Inside a company, people can report bad behavior to management</li>
<li>Or, at the very least, they can take their knowledge of who is a bad actor with them and avoid working or hiring bad actors at other companies</li>
<li>Industries are often fairly small communities. The fear of developing a bad reputation is often a strong motivator for people to behave well.</li>
<li>Because of this, institutions and companies are composed of people that are incentivized to follow implicit codes of ethics.</li>
<li>By default there might be no visibility on what AI workers do inside of an automated institution. Therefore they may have no social forces pushing them to behave well. The automated institution they are part of may thus have no internal forces pushing the institution toward ethical behavior.</li>
</ul>
<h2>Humans prefer to support noble causes</h2>
<ul>
<li>Many people are inspired by noble causes, a desire to do good, and a sense of morality in general</li>
<li>That allows noble causes to have an advantage over dishonorable ones</li>
<li>In a sense, all humans get a vote by choosing who they will work for</li>
<li>In an automated world, the only advantage will go to the cause with more machine resources</li>
</ul>
<h2>Top talent can vote with their feet</h2>
<ul>
<li>The hardest problems in the world require the work of the most talented people in the world</li>
<li>Literal moonshots today can’t succeed without these people, which allows them to “vote” on what moonshots should be “funded” with their talent</li>
<li>Can organized, smart people achieve a Bad Thing on behalf of a self-interested owner? Yes, but they often choose not to, and it certainly is an impediment to evil causes.</li>
<li>Building AI is itself a moonshot. AI researchers have incredible power today to shape the direction of AI, <em>if they choose to wield it</em></li>
</ul>
<h2>People can quit</h2>
<ul>
<li>On the flip side of choosing to work for a cause, people can choose to quit or protest</li>
<li>This limits how nefarious a corporation or government can be</li>
<li>Employees and soldiers are required by law <strong><em>and by our culture </em></strong>to refuse evil orders</li>
<li>Conscientious objection is a powerful limit on government malfeasance</li>
</ul>
<h2>Humans can refuse specific orders</h2>
<ul>
<li>Famously, in 1983, Stanislov Petrov saved the world by refusing to launch nuclear weapons against the United States</li>
<li>There may not be an AI version of Petrov, if the AI is perfectly aligned to do what it’s asked to do</li>
</ul>
<h2>Whistleblowers limit egregious actions</h2>
<ul>
<li>Often leaders preemptively avoid breaking the law because they are afraid someone may whistleblow, not just quit</li>
</ul>
<h2>Conspiracies and cartels are hard to maintain</h2>
<ul>
<li>Conspiracies require concerted effort from many people to succeed</li>
<li>Compliance to the group or cartel becomes exponentially harder as the size of the conspiracy grows. Not true with AI, where compliance (alignment) may be complete.</li>
</ul>
<h2>Cronies are dumb, limiting their impact</h2>
<ul>
<li>Tyrants, mobsters, and would-be dictators need one thing above all else from their henchmen and base of power: loyalty</li>
<li>Often the smartest and most capable refuse to bend the knee, so the tyrant must recruit the less capable instead</li>
<li>The circle of power around the tyrant becomes dumb and ineffective</li>
<li>But with AI, every tyrant may have unfettered intelligence at their disposal, as will their inept cronies.</li>
<li>We should expect to see substantially more capable tyrants and mobsters, powered by AI and unhindered by ethics</li>
</ul>
<h2>Media helps spread knowledge of malfeasance</h2>
<ul>
<li>When someone does have the courage to whistleblow, there are human reporters ready to spread the story</li>
<li>Media corporations can and do collude with nefarious corporate actors and politicians, but a healthy market of many media companies helps ensure someone will spread the story</li>
<li>And the implicit guardrails within media companies help prevent the worst abuses</li>
<li>In an automated world, collusion between a politician and a media owner becomes extremely easy to execute</li>
<li>If the media company is fully automated, it may act on any command from the owner, with no fear of whistleblowers or conscientious objection. Executing a media coverup becomes as simple as the media owner and the politician agreeing to terms.</li>
</ul>
<h2>Social media spreads knowledge that mainstream may not </h2>
<ul>
<li>Even where today’s media fails, every person can pick up and spread a story they see on social media</li>
<li>In a world of infinite machines, indistinguishable from humans, the human choice to amplify will be muted</li>
<li>We’re already seeing this effect from bots online, but today savvy humans can still tell apart human and machine. Tomorrow, it will likely be impossible to discern even for the most savvy among us.</li>
</ul>
<h2>Humans die</h2>
<ul>
<li>The ultimate limit of a human is their lifespan. No matter how much power they accumulate, one day they must pass it on</li>
<li>An AI need not have a lifespan. An empowered AI that faithfully represents one person’s values may enforce those values forever</li>
</ul>
<h2>Limited power of committees</h2>
<ul>
<li>A committee or board may decide something, but the execution of a committee-made decision today is done by other people. The power ultimately lies with those people</li>
<li>You may put a committee in charge of overseeing people that use an AGI toward some ends, but how will the committee hold those people responsible?</li>
<li>What mechanism does the committee have to actually throttle the user of AGI if the user isn’t listening to the committee? Would the committee even know? Does a misused AGI have a responsibility to report back not just to the user, but to the superseding committee the user is acting on behalf of?</li>
<li>Today, any human worker may choose to circumvent their chain of command and inform a committee of misdeeds. Tomorrow, if AIs are perfectly compliant to their user, oversight committees may have no real power</li>
</ul>
<h2>Principal-agent problems stymie large organizations</h2>
<ul>
<li>The principal-agent problem is a well studied management problem, where the goals of an employee (the agent) may not align with the goals of the owner (the principal)</li>
<li>For example, an employee might treat a client or competitor more kindly, because they might work for them in the future</li>
<li>Or, an employee may seek a project that helps them get promoted, even when it’s the wrong project to help the company. Or a trader may take on risks that net out positive for them, but net out negative for the people who gave them their money to trade.</li>
<li>This is a strong limiting factor on the power of large organizations, and is one reason among many why small organizations can often outcompete larger ones. None of these internal misalignments may exist inside automated orgs</li>
</ul>
<h2>Community approval and self-approval influence human actions</h2>
<ul>
<li>People want to do things their loved ones and friends would approve of (and that they themselves can be proud of)</li>
<li>In many ways we’re an honor bound society</li>
<li>This allows for all of society to apply implicit guardrails on all actions, even perfectly hidden actions that no one will ever know about</li>
<li>A soldier wants to act in a way that they can be proud of, or that their family would be proud of. This helps prevent some of the worst abuses in war</li>
<li>Although many abuses nonetheless occur in war, how many more would happen if soldiers perfectly obeyed every order from their general? What if the general knew no one —not even their soldiers— would ever object or tell the world what horrible deeds they did?</li>
<li>Soldiers rarely will agree to fire on civilians, especially their own civilians. An AI soldier that follows orders will have no such compunction</li>
</ul>
<h2>Personal fear of justice</h2>
<ul>
<li>The law applies to individuals, not just organizations, and the fear of breaking the law means a human will often disobey a job or order</li>
<li>But an AI need not have fear</li>
</ul>
<h2>Judges and police officers have their own ethics</h2>
<ul>
<li>The application of law often requires the personal ethical considerations of the judge. Not all law is explicit</li>
<li>That judge is themself a member of society, and feels the social burden of advocating for justice their community would be proud of. This often blunts the force of unjust laws.</li>
<li>Likewise, a police officer will often waive the enforcement of a law if they feel extraneous circumstances warrant it</li>
</ul>
<h2>There’s general friction in enforcement of laws and regulations</h2>
<ul>
<li>We can’t enforce all laws all the time and that’s part of our enforcement model</li>
<li>In the old days a cop needed to be physically present to ticket you for speeding; now in many areas ticketing is end to end automated (right down to mailing the ticket to your home) but speed limits haven’t changed</li>
<li>Our laws are so voluminous and complex, that almost all citizens break the law at some point. Often these infractions go unnoticed by the state. But with perfect automation, every misstep may be noticed.</li>
<li>If automated law enforcement itself reports up to a single stakeholder —as it does today with the President— it would be very easy for that individual to weaponize this power against their political adversaries</li>
</ul>
<h2>Lack of internal competition can slow down big entities</h2>
<ul>
<li>The central point in the theory of capitalism is that we need self-interested competition to align human incentives</li>
<li>This requires having a healthy market, which encourages many multipolar outcomes among industries, spreading out power across society</li>
<li>The reason alternatives to capitalism —like communism— often fail, is that humans lose motivation when you remove their incentives.</li>
<li>AI may not need incentive structures. They may work just as hard on any task we give them, without any need for incentives.</li>
<li>Big human organizations suffer inefficiencies because they have no internal markets or competition correctly driving human incentives, but this won’t be true with AI</li>
</ul>
<h2>The bread and circus isn’t easy to maintain</h2>
<ul>
<li>With full automation, it may be arbitrarily easy to keep a society fed and entertained, even as all other power is stripped from them</li>
<li>Today, to properly feed a society we need a well kept human economy, which requires many more human affordances by necessity. This is one reason why capitalism and liberty have often gone hand in hand.</li>
</ul>
<h2>Leaders can’t execute on their own</h2>
<ul>
<li>We’re seeing the trend today that managers are being more hands on, and need fewer intermediaries</li>
<li>Typically a leader must act through layers of managers to achieve things</li>
<li>But tomorrow, a strong enough technical leader may be able to directly pair with an AGI or superintelligence without any additional assistance from employees</li>
<li>In order to improve security, some AI labs are already isolating which members have access to the next frontier, so it wouldn’t even raise alarm bells for an employee to no longer have access and to be unaware of who does</li>
<li>It will be increasingly easy for a single person to be the only person to have access to a superintelligence, and for no one else to even know this is the case.</li>
</ul>
<h2>Time moves slowly</h2>
<ul>
<li>We expect things to take a long time, which gives us many opportunities to respond, see partial outcomes, and rally a response. AI may move too fast to allow this</li>
<li>Explicitly, we have term limits to our elected offices. This prevents some forms of accumulated power. It also allows citizens to have a feedback loop on timescales that matter</li>
<li>But if AI moves society forward at 10x speed, that's the equivalent of having a president in power for a 40 year term </li>
</ul>
<h2>Geopolitical interdependence disperses power</h2>
<ul>
<li>Nations are interdependent, as are international markets</li>
<li>It’s well understood that no nation can stand alone and isolated</li>
<li>This has a mediating force on international politics and helps ensure peace is a mutually beneficial outcome</li>
<li>In an automated world, nations may have everything they need domestically and lose this implicit need to peacekeep with their peers</li>
</ul>
<h2>An army of the willing will only fight for certain causes</h2>
<ul>
<li>Outright war is extremely unpopular because it compels citizens to fight and die</li>
<li>Automated wars may be unpopular, but not nearly as unpopular if citizens are insulated from the fighting</li>
<li>We already see this effect with our ability to wage war from the sky</li>
</ul>
<h2>An interdependent corporate ecosystem disperses power</h2>
<ul>
<li>A corporation is dependent on a much larger ecosystem</li>
<li>To continue growing, large companies must play by the rules within that ecosystem</li>
<li>That interdependence creates a multipolar power distribution among even the most successful companies</li>
<li>Full vertical integration is nearly impossible today, but may not be tomorrow</li>
</ul>
<h2>Surveillance is hard</h2>
<ul>
<li>We’ve had the ability to record every form of communication for decades</li>
<li>But <em>analyzing</em> all communication has required an infeasible amount of human power</li>
<li>With AI, we (or tyrants) will have unlimited intelligence to analyze the meaning of every text message, phone call, and social media post for any implied threats or disloyalties</li>
<li>This is already happening in CCP-controlled China</li>
</ul>
<h2>Elite social pressure matters to many leaders</h2>
<ul>
<li>Even leaders have a community they often feel beholden to: the elites</li>
<li>Elites do have some ability to informally influence leaders, even dictators</li>
<li>But elites can be fully captured by leaders. Stalin and Hitler succeeded at this even with primitive tech. With the power of full automation this may be even easier.</li>
</ul>
<h2>In the final limit, citizens can revolt</h2>
<ul>
<li>Even the most authoritarian governments have to consider the risk of pushing the polity beyond the breaking point</li>
<li>That breaking point has historically been very far, but even the threat of it has served as a metering force on rulers</li>
<li>There may be no such limit in the future</li>
</ul>
<h2>Humans have economic and strategic value</h2>
<ul>
<li>Authoritarians can’t simply kill all their citizens today, or their economy and warmaking ability would be gutted. In fact, they are incentivized to create a rich economy, in order to have doctors, entertainment, and luxuries.</li>
<li>The Khmer Rouge killed nearly 25% of their own population, crippling their own warmaking ability. They ended up obliterated by a Vietnamese invasion.</li>
<li>Even the most psychopathic ruler, if self-interested, must support their people to support themself </li>
<li>But post-AGI, from the point of view of a dictator, what’s the point of supporting other humans with their national output at all? To them, citizens might become economic deadweight</li>
<li>And even if one authoritarian wants to support their population, another authoritarian who doesn’t will likely outcompete them across relevant domains</li>
</ul>
<h2>Even dictators need their citizens</h2>
<ul>
<li>With AI and a fully automated economy, this will no longer be true </li>
</ul>
<h2>Replacing implicit guardrails with explicit design</h2>
<p>AI has the potential for tremendous upside, the point of this exercise isn’t to paint AI in a negative light. Instead, it’s to highlight that AI will reshape our society at every level, and that will require rethinking the way every level works. Our society is saturated with implicit guardrails. If we removed them all without replacing them with new guardrails, society would almost surely collapse. Moreover, the explicit guardrails we do have today —our laws and explicit institutions— have been designed with our existing implicit guardrails in mind. They’re complimentary.</p>
<p>We have to think carefully about how a new, automated world will work. We need to consider what values we want that world to exemplify. We need to reconsider preconceived design patterns that worked when implicit guardrails were strong, but may stop working when those guardrails disappear. We have to discover a new set of explicit guardrails that will fortify our freedoms against what is to come.</p>
<p>And we must do this preemptively.</p>
<p>Humans are fantastic at iterating. We observe our failures and continue to modify our approach until we succeed. We’ve done this over thousands of years to refine our societies and guardrails. We’ve been successful enough to prevent the worst among us from seizing absolute power. But the transition to an automated world may happen over the course of a few years, not thousands of years. And we may not recover from the failures. There may not be a chance to iterate.</p>
<p>If our pervasive, implicit guardrails disappear all at once, the nefarious forces they’ve held at bay may overwhelm us decisively. To survive we must design an explicit set of guardrails to safeguard the future.</p>

        </div>
        <div class="back-link">
            <a href="/">&larr; Back</a>
        </div>
    </div>
</body>
</html>